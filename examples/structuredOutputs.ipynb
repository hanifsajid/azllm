{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bf482f",
   "metadata": {},
   "source": [
    "## Structured Outputs (Parsed)\n",
    "\n",
    "- Natively, only the following models/providers support structured outputs: OpenAI, Grok, Gemini, Fireworks, and Ollama. \n",
    "- Structured outputs can also be enabled using `azllm` for: DeepSeek and Anthropic.\n",
    "    - If parsing into the Pydantic model fails (e.g. invalid structure or missing fields), the system automatically falls back to returning the raw unstructured response. \n",
    "- To enable structured output parsing, set `parse=True`.\n",
    "- A `Pydantic` model passed via `response_format` in kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96ade22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azllm import azLLM\n",
    "manager = azLLM()  # Instantiated with default parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ce9b7",
   "metadata": {},
   "source": [
    "\n",
    "### Parsed Responses with `generate_text` Method\n",
    "\n",
    "The `generate_text`method optionally parse the response from a single prompt using a specified model.\n",
    "\n",
    "- Accepts a kwargs dictionary for `response_format` and any other additonal additional options (e.g., max_tokens, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d04b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class Capital(BaseModel):\n",
    "    capital: str = Field(..., description=\"Capital name only\")\n",
    "\n",
    "\n",
    "prompt = 'What is the captial of France?'\n",
    "generated_text = manager.generate_text('gemini', prompt, kwargs={'response_format': Capital}, parse=True)\n",
    "str_output = generated_text.parsed.capital\n",
    "print(str_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b004c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "generated_text = manager.generate_text('deepseek', prompt, kwargs={'response_format': Capital}, parse=True)\n",
    "str_output = generated_text.parsed.capital\n",
    "print(str_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61015ff",
   "metadata": {},
   "source": [
    "### Parsed Responses with `batch_generate` Method\n",
    "\n",
    "\n",
    "The `batch_generate` method optionally parse outputs from a model for multiple prompts in one call.\n",
    "\n",
    "- For each prompt, you can pass a corresponding dictionary of keyword arguments (kwargs).\n",
    "- Each kwargs dictionary may include:\n",
    "    - A response_format: a Pydantic model class used to parse the structured output.\n",
    "    - Other model parameters like max_tokens, temperature, etc.\n",
    "- The kwargs argument itself is a list — each dictionary in the list aligns with a prompt in batch_prompts.\n",
    "- Use the parse argument (a list of booleans) to specify whether the output for each prompt should be parsed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4dbd51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "________________________________________\n",
      "Why don't scientists trust atoms? Because they make up everything!\n",
      "________________________________________\n",
      "The capital of the United States is Washington, D.C.\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class Capital(BaseModel):\n",
    "    capital: str = Field(..., description=\"Capital name only\")\n",
    "class Joke(BaseModel):\n",
    "    joke: str = Field(..., description=\"A simple one or two sentences joke\" )\n",
    "\n",
    "batch_prompts = [\n",
    "    'What is the capital of France?',\n",
    "     'Tell me a joke.', \n",
    "     'What is the capital of USA?'\n",
    "     ]\n",
    "\n",
    "results = manager.batch_generate('openai', batch_prompts,\n",
    "                                 kwargs= [{'response_format': Capital},\n",
    "                                 {'response_format': Joke, 'max_tokens': 200}, {'max_tokens': 100}], parse=[True, True, False])\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    if idx ==0:\n",
    "        print(result.parsed.capital)\n",
    "    elif idx == 1:\n",
    "        print(result.parsed.joke)\n",
    "    elif idx == 2:\n",
    "        print(result)\n",
    "    print(\"_\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2242a",
   "metadata": {},
   "source": [
    "### Parsed Responses with `parallel_generation` Method\n",
    "\n",
    "\n",
    "The `generate_parallel` method allows generating responses from multiple models in parallel, with optional structured parsing for each model.\n",
    "\n",
    "You can choose whether or not to parse the output for each model individually.\n",
    "This is useful for comparing how models behave with and without structured parsing for the same prompt.\n",
    "\n",
    "- kwargs: A list of dictionaries aligned with each model. Each can include:\n",
    "    - response_format: A Pydantic model for parsing.\n",
    "    - Other generation parameters like max_tokens, temperature, etc.\n",
    "- parse: A list of booleans indicating whether to parse the response for each model.\n",
    "- The result is a dictionary where each key is a model name with an index (e.g., \"grok:1\"), and each value is the corresponding response — parsed or raw depending on the parse flag.\n",
    "- The index after the last colon (e.g., :1) indicates the model's position in the clients_models list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41aa1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: grok:1\n",
      "Parsed Final Answer: The solution to the equation 8x + 7 = -23 is x = -3.75.\n",
      "\n",
      "Client: openai:0\n",
      "Raw Result: To solve the equation \\( 8x + 7 = -23 \\), follow these steps:\n",
      "\n",
      "1. **Subtract 7 from both sides** to isolate the term with \\( x \\):\n",
      "   \\[\n",
      "   8x + 7 - 7 = -23 - 7\n",
      "   \\]\n",
      "   Simplifying this gives:\n",
      "   \\[\n",
      "   8x = -30\n",
      "   \\]\n",
      "\n",
      "2. **Divide both sides by 8** to solve for \\( x \\):\n",
      "   \\[\n",
      "   x = \\frac{-30}{8}\n",
      "   \\]\n",
      "   This simplifies to:\n",
      "   \\[\n",
      "   x = -\\frac{15}{4} \\quad \\text{or} \\quad x = -3.75\n",
      "   \\]\n",
      "\n",
      "So the solution to the equation \\( 8x + 7 = -23 \\) is \\( x = -\\frac{15}{4} \\) or \\( x = -3.75 \\).\n",
      "\n",
      "Client: grok:2\n",
      "Raw Result: To solve the equation \\(8x + 7 = -23\\), follow these steps:\n",
      "\n",
      "1. **Isolate the term with the variable \\(x\\)**:\n",
      "   Subtract 7 from both sides of the equation to get rid of the constant term on the left side.\n",
      "   \\[\n",
      "   8x + 7 - 7 = -23 - 7\n",
      "   \\]\n",
      "   Simplifying this, we get:\n",
      "   \\[\n",
      "   8x = -30\n",
      "   \\]\n",
      "\n",
      "2. **Solve for \\(x\\)**:\n",
      "   Divide both sides of the equation by 8 to isolate \\(x\\).\n",
      "   \\[\n",
      "   \\frac{8x}{8} = \\frac{-30}{8}\n",
      "   \\]\n",
      "   Simplifying this, we get:\n",
      "   \\[\n",
      "   x = -\\frac{30}{8}\n",
      "   \\]\n",
      "   You can simplify \\(-\\frac{30}{8}\\) by dividing the numerator and the denominator by their greatest common divisor, which is 2:\n",
      "   \\[\n",
      "   x = -\\frac{15}{4}\n",
      "   \\]\n",
      "\n",
      "So, the solution to the equation \\(8x + 7 = -23\\) is \\(x = -\\frac{15}{4}\\).\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the expected structure for parsed output\n",
    "class MathReasoning(BaseModel):\n",
    "    class Step(BaseModel):\n",
    "        explanation: str\n",
    "        output: str\n",
    "\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "\n",
    "# Prompt to be sent to all model clients\n",
    "question = \"How can I solve 8x + 7 = -23?\"\n",
    "\n",
    "# List of models to query in parallel\n",
    "clients_models = ['openai', 'grok', 'grok']\n",
    "\n",
    "# Per-model keyword arguments\n",
    "kwargs = [\n",
    "    {},  # No parsing for openai\n",
    "    {'response_format': MathReasoning},  # Enable parsing for grok\n",
    "    {}  # No parsing for second grok\n",
    "]\n",
    "\n",
    "# Whether or not to parse the response from each model\n",
    "parse_flags = [False, True, False]\n",
    "\n",
    "# Generate responses in parallel\n",
    "results = manager.generate_parallel(\n",
    "    prompt=question,\n",
    "    clients_models_versions=clients_models,\n",
    "    kwargs=kwargs,\n",
    "    parse=parse_flags\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "for client_model, result in results.items():\n",
    "    print(f\"\\nClient: {client_model}\")\n",
    "\n",
    "    if getattr(result, 'parsed', None):\n",
    "        # If parsed, print the structured final answer\n",
    "        print(f\"Parsed Final Answer: {result.parsed.final_answer}\")\n",
    "    else:\n",
    "        # Otherwise, print the raw response\n",
    "        print(f\"Raw Result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azllm-I_5BKfWE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
